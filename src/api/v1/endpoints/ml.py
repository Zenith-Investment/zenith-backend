"""
ML endpoints for predictions and model management.
"""

from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
import logging

from ....ml.prediction.predictor import Predictor
from ....ml.training.trainer import ModelTrainer
from ....ml.models.model_registry import ModelRegistry

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/ml", tags=["ml"])

# Initialize ML components
predictor = Predictor(
    models_path="/app/models",
    redis_url="redis://redis:6379/0",
    use_cache=True
)

trainer = ModelTrainer(models_path="/app/models")
registry = ModelRegistry(registry_path="/app/models/registry.json")


# Pydantic models
class PredictionRequest(BaseModel):
    ticker: str = Field(..., description="Stock ticker symbol (e.g., PETR4.SA)")
    horizon: int = Field(5, ge=1, le=30, description="Prediction horizon in days")
    use_cache: bool = Field(True, description="Whether to use cached predictions")


class SignalRequest(BaseModel):
    ticker: str = Field(..., description="Stock ticker symbol")
    use_cache: bool = Field(True, description="Whether to use cached predictions")


class TrainRequest(BaseModel):
    ticker: str = Field(..., description="Stock ticker symbol to train models for")
    model_type: Optional[str] = Field(None, description="Model type (lstm, ensemble, or all)")
    epochs: Optional[int] = Field(100, ge=10, le=500, description="Number of epochs for LSTM")


class PredictionResponse(BaseModel):
    ticker: str
    model_type: str
    current_price: float
    predictions: List[Dict[str, Any]]
    horizon: int
    timestamp: str
    confidence: str


class SignalResponse(BaseModel):
    ticker: str
    model_type: str
    signal: str
    confidence: float
    current_price: float
    timestamp: str
    probabilities: Optional[Dict[str, float]] = None


class ModelInfo(BaseModel):
    model_type: str
    ticker: str
    version_id: str
    metrics: Dict[str, float]
    registered_at: str
    status: str


# Endpoints

@router.post("/predict/price", response_model=PredictionResponse)
async def predict_price(request: PredictionRequest):
    """
    Predict future stock prices using LSTM model.

    **⚠️ DISCLAIMER**: This prediction is for educational purposes only and does not constitute
    financial advice. Past performance does not guarantee future results.
    """
    try:
        # Load models if not loaded
        if predictor.lstm_model is None:
            latest_model = registry.get_latest_model("lstm", request.ticker)
            if latest_model:
                predictor.load_models(lstm_path=latest_model["model_path"])
            else:
                raise HTTPException(
                    status_code=404,
                    detail=f"No trained LSTM model found for {request.ticker}"
                )

        # Make prediction
        result = predictor.predict_price(
            ticker=request.ticker,
            horizon=request.horizon,
            use_cache=request.use_cache
        )

        return PredictionResponse(**result)

    except Exception as e:
        logger.error(f"Error predicting price for {request.ticker}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/predict/signal", response_model=SignalResponse)
async def predict_signal(request: SignalRequest):
    """
    Predict trading signal (buy/hold/sell) using ensemble model.

    **⚠️ DISCLAIMER**: This signal is generated by ML models and should not be the sole basis
    for investment decisions. Always conduct your own research and consult with financial advisors.
    """
    try:
        # Load models if not loaded
        if predictor.ensemble_model is None:
            latest_model = registry.get_latest_model("ensemble", request.ticker)
            if latest_model:
                predictor.load_models(ensemble_path=latest_model["model_path"])
            else:
                raise HTTPException(
                    status_code=404,
                    detail=f"No trained ensemble model found for {request.ticker}"
                )

        # Make prediction
        result = predictor.predict_signal(
            ticker=request.ticker,
            use_cache=request.use_cache
        )

        return SignalResponse(**result)

    except Exception as e:
        logger.error(f"Error predicting signal for {request.ticker}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/predict/comprehensive")
async def predict_comprehensive(request: PredictionRequest):
    """
    Get comprehensive prediction including both price forecast and trading signal.

    **⚠️ DISCLAIMER**: These predictions are for informational and educational purposes only.
    They do not constitute investment advice. Trading stocks involves risk of loss.
    """
    try:
        # Load models
        lstm_model = registry.get_latest_model("lstm", request.ticker)
        ensemble_model = registry.get_latest_model("ensemble", request.ticker)

        if lstm_model:
            predictor.load_models(lstm_path=lstm_model["model_path"])
        if ensemble_model:
            predictor.load_models(ensemble_path=ensemble_model["model_path"])

        # Make comprehensive prediction
        result = predictor.predict_comprehensive(
            ticker=request.ticker,
            horizon=request.horizon,
            use_cache=request.use_cache
        )

        return result

    except Exception as e:
        logger.error(f"Error making comprehensive prediction for {request.ticker}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/train")
async def train_models(
    request: TrainRequest,
    background_tasks: BackgroundTasks
):
    """
    Train ML models for a specific ticker.

    This is a long-running operation that will be executed in the background.
    """
    try:
        def train_task():
            try:
                if request.model_type == "lstm" or request.model_type is None:
                    logger.info(f"Training LSTM model for {request.ticker}")
                    model, metrics, model_path = trainer.train_lstm_model(
                        ticker=request.ticker,
                        epochs=request.epochs,
                        save_model=True
                    )

                    # Register model
                    registry.register_model(
                        model_type="lstm",
                        model_path=model_path,
                        ticker=request.ticker,
                        metrics=metrics
                    )

                if request.model_type == "ensemble" or request.model_type is None:
                    logger.info(f"Training ensemble model for {request.ticker}")
                    model, metrics, model_path = trainer.train_ensemble_model(
                        ticker=request.ticker,
                        save_model=True
                    )

                    # Register model
                    registry.register_model(
                        model_type="ensemble",
                        model_path=model_path,
                        ticker=request.ticker,
                        metrics=metrics
                    )

                logger.info(f"Training completed for {request.ticker}")

                # Invalidate cache
                predictor.invalidate_cache(request.ticker)

            except Exception as e:
                logger.error(f"Training failed for {request.ticker}: {e}")

        # Schedule training task
        background_tasks.add_task(train_task)

        return {
            "message": f"Training started for {request.ticker}",
            "model_type": request.model_type or "all",
            "status": "queued"
        }

    except Exception as e:
        logger.error(f"Error starting training for {request.ticker}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/models", response_model=List[ModelInfo])
async def list_models(
    ticker: Optional[str] = None,
    model_type: Optional[str] = None,
    status: str = "active"
):
    """
    List available ML models.
    """
    try:
        stats = registry.get_stats()
        models = []

        for key, model_list in registry.registry.items():
            for model in model_list:
                if model.get("status") != status:
                    continue

                if ticker and model["ticker"] != ticker:
                    continue

                if model_type and model["model_type"] != model_type:
                    continue

                models.append(ModelInfo(
                    model_type=model["model_type"],
                    ticker=model["ticker"],
                    version_id=model["version_id"],
                    metrics=model["metrics"],
                    registered_at=model["registered_at"],
                    status=model["status"]
                ))

        return models

    except Exception as e:
        logger.error(f"Error listing models: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/cache/stats")
async def get_cache_stats():
    """
    Get cache statistics.
    """
    try:
        stats = predictor.get_cache_stats()
        return stats
    except Exception as e:
        logger.error(f"Error getting cache stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/cache/{ticker}")
async def invalidate_cache(ticker: str):
    """
    Invalidate cache for a specific ticker.
    """
    try:
        predictor.invalidate_cache(ticker)
        return {"message": f"Cache invalidated for {ticker}"}
    except Exception as e:
        logger.error(f"Error invalidating cache for {ticker}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/cache")
async def clear_all_cache():
    """
    Clear all prediction cache.
    """
    try:
        predictor.invalidate_cache()
        return {"message": "All cache cleared"}
    except Exception as e:
        logger.error(f"Error clearing cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/disclaimer")
async def get_disclaimer():
    """
    Get legal disclaimer for ML predictions.
    """
    return {
        "disclaimer": """
        ⚠️ IMPORTANT LEGAL DISCLAIMER

        The predictions, signals, and analysis provided by this Machine Learning system are for
        EDUCATIONAL AND INFORMATIONAL PURPOSES ONLY and should not be construed as financial,
        investment, or trading advice.

        KEY POINTS:

        1. NO INVESTMENT ADVICE: Nothing on this platform constitutes a recommendation to buy,
           sell, or hold any security or investment.

        2. NOT GUARANTEED: Past performance and model predictions do not guarantee future results.
           Stock markets are inherently unpredictable and involve substantial risk of loss.

        3. MODEL LIMITATIONS: Machine Learning models have limitations and may produce incorrect
           predictions. Accuracy rates shown are based on historical data and may not reflect
           future performance.

        4. DO YOUR OWN RESEARCH: Always conduct your own thorough research and analysis before
           making any investment decisions.

        5. CONSULT PROFESSIONALS: Consider consulting with qualified financial advisors before
           making investment decisions.

        6. RISK OF LOSS: Trading and investing in stocks involves risk of loss. You may lose some
           or all of your capital.

        7. NO LIABILITY: The creators, operators, and contributors to this platform accept no
           responsibility or liability for any losses or damages arising from the use of this
           information.

        By using this ML prediction service, you acknowledge that you understand and accept these
        terms and that you will not hold the platform or its operators liable for any trading
        losses or damages.

        If you do not agree with these terms, please do not use this service.
        """,
        "version": "1.0",
        "last_updated": "2025-01-28"
    }
